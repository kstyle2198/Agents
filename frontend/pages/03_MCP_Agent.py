import streamlit as st
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_groq import ChatGroq
from langchain_core.messages import AIMessage
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# --- ìºì‹±ì„ ì‚¬ìš©í•œ ë¹„ë™ê¸° í•¨ìˆ˜ ì„¤ì • ---

@st.cache_resource(show_spinner="Connecting to Tool-Server...")
def get_tools():
    """
    MCP ì„œë²„ì— ì—°ê²°í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    Streamlitì˜ @st.cache_resourceë¥¼ ì‚¬ìš©í•˜ì—¬ ì•± ì„¸ì…˜ ë™ì•ˆ ë„êµ¬ ëª©ë¡ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    server_config = {
        "search": {
            "url": "http://localhost:8000/mcp",
            "transport": "sse",
        },
    }
    
    async def fetch_tools():
        client = MultiServerMCPClient(server_config)
        return await client.get_tools()

    # Streamlitì˜ ë™ê¸°ì  ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    try:
        return asyncio.run(fetch_tools())
    except Exception as e:
        st.error(f"Failed to connect to the tool server: {e}")
        return []

# --- Streamlit UI ì„¤ì • ---
st.set_page_config(page_title="UI", page_icon="ğŸ¬", layout="wide", initial_sidebar_state="collapsed")
st.title("MCP Agent")
st.caption("A Streamlit app for LangGraph ReAct agent with MCP Tools")

# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ë¡œë“œ ë° í‘œì‹œ
tools = get_tools()
if tools:
    tool_names = [tool.name for tool in tools]
    st.sidebar.success("âœ… Tool-Server Connected")
    st.sidebar.write("Available Tools:")
    for name in tool_names:
        st.sidebar.code(name, language="text")
else:
    st.sidebar.error("âŒ Tool-Server Disconnected")
    st.warning("Tool server is not available. The agent might not function correctly.")

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ì—ì´ì „íŠ¸ ë° ëª¨ë¸ ì„¤ì • ---

# LLM ì •ì˜
model = ChatGroq(
    model="qwen/qwen3-32b", # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥
    temperature=0.5,
    max_tokens=2000,
)

# í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt_template = """
You are the Smart AI Assistant in a company.
Based on the result of tool calling, Generate a consice and logical answer.
and if there is no relevant infomation in the tool calling result, Just say 'I don't know'.
Answer in Korean.
"""

# ReAct ì—ì´ì „íŠ¸ ìƒì„±
# toolsê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
if tools:
    agent = create_react_agent(model=model, tools=tools, prompt=prompt_template)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        
        try:
            # LangGraph ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¼ í˜¸ì¶œ
            inputs = {"messages": [("user", user_query)]}
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
            async def stream_and_get_response():
                # ì´ í•¨ìˆ˜ ë‚´ì˜ ì§€ì—­ ë³€ìˆ˜ë¡œ full_responseë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                _full_response = "" 
                
                # ainvokeëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                async for chunk in agent.astream(inputs, stream_mode="values"):
                    # ìŠ¤íŠ¸ë¦¼ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€(AIMessage)ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    if "messages" in chunk and isinstance(chunk["messages"][-1], AIMessage):
                        message_content = chunk["messages"][-1].content
                        # ì´ì „ ë‚´ìš©ê³¼ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ ê¹œë¹¡ì„ ë°©ì§€
                        if message_content != _full_response:
                            _full_response = message_content
                            response_placeholder.markdown(_full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼ ì¶”ê°€
                
                response_placeholder.markdown(_full_response) # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                return _full_response # ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

            # Streamlitì—ì„œ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼ì„ ì‹¤í–‰í•˜ê³ , ë°˜í™˜ëœ ê°’ì„ full_responseì— ì €ì¥í•©ë‹ˆë‹¤.
            full_response = asyncio.run(stream_and_get_response())

        except Exception as e:
            st.error(f"An error occurred: {e}")
            full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            response_placeholder.markdown(full_response)

    # ìµœì¢… ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": full_response})